## Discussion

The rapid progress of current-generation Large Language Models (LLMs) presents a significant challenge to society and the biomedical field specifically (Nature, 2024; Nature, 2023; Nature, 2023).
While these models hold immense potential, their implementation is complex and requires expertise (Nature, 2023).
Furthermore, biomedical research is often conducted in isolation due to the intricate nature of the domain and incentives that hinder open collaboration (Nature, 2012; Nature, 2024).
Taking inspiration from successful open-source platforms like LangChain, we suggest an open framework for biomedical researchers to focus on utilizing LLMs rather than dealing with technical obstacles.
To ensure the framework's effectiveness and longevity, we leverage existing open-source tools while incorporating innovations from the broader LLM community into the biomedical field.
Emphasizing transparency at each stage of the framework is crucial for the sustainable application of LLMs in biomedical research and beyond (Nature, 2024).

Efficient communication between humans and artificial intelligence may benefit from a common language: symbolic representations of concepts that are easily understood during conversation (Smith, 2019).
Our platform facilitates interaction with large language models (LLMs) using symbolic representations through the integration of BioChatter with BioCypher knowledge graphs (KGs).
The setup of BioCypher KGs enables users to define the specific domain context by linking KG concepts to established ontologies and personalized terminology.
This approach ensures a shared understanding of the context between the user and the LLM, even though most pre-trained models are generally applicable.

We prioritize the robustness and objective evaluation of Large Language Models (LLMs) within our framework.
To achieve this, we have established a dynamic benchmarking system that enables the automated assessment of LLMs, prompts, and other components (https://biochatter.org/benchmark/).
Existing biomedicine-specific benchmarking initiatives are often limited in scope and rely on manual processes that overlook various component combinations.
Additionally, many benchmarks utilize web interfaces for LLMs, which can obscure crucial parameters like model version and temperature [@biollmbench].
Therefore, implementing a framework is crucial for ensuring the unbiased and replicable evaluation of LLMs.
We safeguard against data leakage from benchmark datasets into the training data of new models through encryption, ensuring the longevity of the benchmark as new models are introduced.
The dynamic benchmark will continue to evolve by incorporating new questions and tasks relevant to the community.

The results of the benchmark provide useful criteria for selecting models and serve as a starting point for investigating variations in performance.
For example, the benchmark quickly highlighted a decrease in performance when transitioning from the older (0613) to the newer (0125) version of GPT-4.
It also identified several pre-trained open-source models that are suitable for our purposes, particularly the openhermes-2.5 model with 4- or 5-bit quantization.
This model, a fine-tuned variant of Mistral 7B v0.1 on GPT-4-generated data, outperformed the vanilla variants in our benchmarks.
Notably, BioChatter was created using GPT-3.5-turbo-0613, and to a lesser extent, GPT-4-0613 and LLAMA-2-CHAT (13B); as a result, the development of BioChatter did not impact the benchmark performance of models like openhermes-2.5 and the newer GPT models.

We make it easier to use LLMs by allowing access to both proprietary and open-source models.
Our framework offers a flexible deployment option for open-source models.
Proprietary models are currently the most cost-effective choice for accessing the latest models and are ideal for beginners or those with limited resources.
On the other hand, open-source models are rapidly improving in performance and are crucial for the long-term viability of the field (Biollmbench).
Our platform enables users to host open-source models themselves, whether on dedicated hardware with GPUs, locally on their laptops, or through browser-based deployment using web technology.

### Limitations

The current generation of Large Language Models (LLMs) is not yet suitable for unsupervised use in biomedical research due to the vast array of unique subfields within the field.
Effectively supporting this diversity through robust and contextually aware interactions with LLMs is a challenging task.
While efforts have been made to reduce risks associated with using LLMs, such as independent benchmarks, fact-checking, and Retrieval-Augmented Generation (RAG) processes, there is no guarantee that the models will not produce harmful outputs.
Current LLMs, particularly within the BioCypher ecosystem, are viewed as tools to support human researchers by assisting with menial and repetitive tasks, as well as technical aspects like query languages.
They are not intended to replace human ingenuity and expertise, but rather to enhance it with their complementary strengths.
Despite the user-friendly design of BioChatter, researchers unfamiliar with LLMs or the specific functionalities of the framework may experience a learning curve.
To maximize the benefits of BioChatter to the community, it will be crucial to encourage adoption and provide adequate training and support.

### Future directions

Emerging research shows that multitask learners, capable of synthesizing language, vision, and molecular measurements, are gaining traction [@doi:10.48550/arXiv.2306.04529;@doi:10.48550/arXiv.2211.01786;@doi:10.48550/arXiv.2310.09478].
Some autonomous agents for basic tasks have already been created using Large Language Models (LLMs), and we anticipate further advancements in this area [@doi:10.48550/arXiv.2308.11432].
As studies on multimodal learning and agent behavior advance, we aim to incorporate these findings into the BioChatter framework.
Our framework development prioritizes ethical considerations related to LLMs, emphasizing the use of open-source models to enhance transparency and data privacy.
Although our primary focus is on biomedicine, the adaptability of our frameworks allows for easy extension to other scientific fields through adjustments to domain-specific prompts and data inputs, which are conveniently accessible in our frameworks [@biocypher].
Our Python library is openly developed on GitHub ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) and can be seamlessly integrated into various user interface solutions.
Operating under the permissive MIT license, we welcome community contributions and suggestions for integrating bioinformatics tools, prompt engineering, benchmarking, and other features.
