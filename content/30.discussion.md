## Discussion

The rapid advancements in current Large Language Models (LLMs) present significant challenges, not only to society at large but also specifically to the biomedical field.
These challenges are documented in several studies [@doi:10.1038/d41586-024-00029-4;@doi:10.1038/d41586-023-03817-6;@doi:10.1038/d41586-023-03803-y].
Although these models hold great promise, utilizing them is far from simple and requires specialized knowledge [@doi:10.1038/s41587-023-02103-0].
Moreover, biomedical research often happens in isolation, a situation exacerbated by the field's complexity and systemic barriers to open science and collaboration [@doi:10.1177/1745691612459058;@doi:10.1038/d41586-024-00322-2].
Drawing inspiration from the success of open-source libraries like LangChain [@langchain], we introduce an open framework designed to help biomedical researchers apply LLMs without having to overcome technical hurdles.
Our approach involves leveraging existing open-source libraries and tools, while tailoring the latest developments from the broader LLM field for biomedical uses.
We stress transparency at every stage of our framework, believing it to be crucial for the responsible and sustainable use of LLMs in biomedical research and other areas [@doi:10.1038/d41586-024-00029-4].

For effective communication between humans and AI, it's essential to have a common language or "lingua franca," which involves using symbolic representations of concepts that both parties can understand, at least on a basic level (Smith et al., 2022).
Our approach facilitates this interaction with large language models (LLMs) by integrating BioChatter with BioCypher Knowledge Graphs (KGs), thereby grounding discussions in a structured context.
Specifically, BioCypher KGs are designed to let users define the scope of their conversation by linking concepts from the KGs to established ontologies and personalized terminology.
This method ensures that both the user and the LLM share a common understanding of the topic, despite the broad applicability of most pre-trained models.

In our work, we prioritize the importance of thorough and unbiased testing of large language models (LLMs) and their integration within our framework.
To accomplish this, we have developed an innovative benchmarking system that facilitates automatic assessments of LLMs, prompts, and other elements involved (https://biochatter.org/benchmark/).
This approach stands in contrast to the more traditional, smaller-scale manual evaluations seen in recent biomedicine-focused studies.
These conventional methods often fail to explore the vast array of possible interactions between different components and typically rely on web interfaces for LLMs, which can hide crucial information like the model's version and settings [@biollmbench].
Our framework represents a vital step forward in achieving consistent and transparent evaluations of LLMs.
We also address the issue of data integrity by encrypting the benchmark datasets, preventing any unintentional inclusion of this data in the training sets of new models.
This measure is critical for maintaining the relevance and effectiveness of our benchmarking system as it evolves, with updates including new questions and tasks reflective of emerging interests within the community.

The results from our benchmark tests offer clear guidelines for choosing models and provide insight into why some models might not perform as we anticipated.
For example, the benchmark tests quickly highlighted a decrease in performance when comparing an older version of the GPT-4 model (0613) to a newer version (0125).
Furthermore, these tests helped us find a variety of pre-trained models that meet our needs, especially the openhermes-2.5 model, which operates in either 4- or 5-bit quantization.
This particular model, which has been fine-tuned using data generated by GPT-4, is a modified version of the Mistral 7B v0.1.
The original versions of Mistral performed significantly lower in our tests.
It's important to note that the development of BioChatter, another tool in our study, was based on the use of GPT-3.5-turbo-0613, GPT-4-0613, and, to a lesser extent, llama-2-chat (13B).
Therefore, the benchmark results for models like openhermes-2.5 and the newer versions of GPT were not affected by the development of BioChatter.

In our work, we make it easier for users to access large language models (LLMs) by supporting both proprietary and open-source models, offering a versatile framework for the latter.
Proprietary models currently offer a cost-effective way to utilize cutting-edge technology.
This makes them an attractive option for newcomers or those without the means to set up their own models.
However, open-source models are rapidly improving in performance and are becoming competitive with their proprietary counterparts, as highlighted in recent benchmarks [@biollmbench].
Moreover, the adoption of open-source models is crucial for the long-term health of the field, ensuring that advancements remain accessible to a broad community [@doi:10.1038/d41586-024-00029-4].
Our platform supports the self-hosting of open-source models at various scales, including on specialized hardware with GPUs, on local machines such as laptops, and even through web browsers using internet technologies.
This flexibility ensures that users at different levels of technical expertise and with varying resources can leverage the power of LLMs for biomedical applications.

### Limitations

The latest generation of Large Language Models (LLMs) is not fully prepared for direct, unsupervised application within the broad and complex field of biomedical research, which encompasses a wide range of specialized areas.
Effectively catering to this diversity by ensuring that LLM interactions are robust and sensitive to context presents a significant challenge.
Although we have implemented measures to reduce the risks associated with LLM usage—including independent evaluations, fact-checking procedures, and Retrieval-Augmented Generation (RAG) processes—we cannot fully ensure that the models will be free from generating potentially harmful information.
In the context of the BioCypher ecosystem, we view the current LLMs as valuable tools designed to support human researchers.
They are intended to ease the burden of routine and repetitive tasks and to assist with technical matters, such as working with query languages.
Their role is not to supplant the creativity and expertise of humans but to enhance it by offering strengths that complement those of human researchers.
Despite the user-friendly interface of BioChatter, there might be a learning curve for researchers who are not yet familiar with LLMs or the specific features of this framework.
To maximize its usefulness to the community, it will be essential to promote its adoption and to provide comprehensive training and support.

### Future directions

The field of multitask learning, which combines language, vision, and molecular data, is rapidly growing, with significant advancements already made [@doi:10.48550/arXiv.2306.04529;@doi:10.48550/arXiv.2211.01786;@doi:10.48550/arXiv.2310.09478].
Similarly, the development of autonomous agents for simple tasks using large language models (LLMs) shows promise, and it's expected that this area will continue to evolve [@doi:10.48550/arXiv.2308.11432].
In light of these advancements, our intention is to incorporate these innovative approaches into the BioChatter framework, focusing on multimodal learning and agent behavior.
We are committed to considering the ethical aspects of LLMs in all developments and to promoting the use of open-source models to enhance transparency and data protection.
Although our primary focus is on biomedicine, the principles behind our framework can be adapted for use in other scientific fields by modifying the specific prompts and data inputs.
These modifications are designed to be straightforward and user-friendly within our framework [@biocypher].
Our Python library, which is openly available on GitHub at [https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter), can be easily incorporated into any user interface.
It is released under the MIT license, and we welcome community contributions, including the addition of bioinformatics tools, prompt engineering techniques, benchmarking efforts, and any other features that could improve the framework.
