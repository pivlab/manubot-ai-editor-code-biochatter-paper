## Results

BioChatter ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) is a Python framework that provides an easy-to-use interface to interact with LLMs and auxiliary technologies via an intuitive API (application programming interface).
This way, its functionality can be integrated into any number of user interfaces, such as web apps, command-line interfaces, or Jupyter notebooks (Figure @fig:architecture).

The framework is designed to be modular, allowing for the exchange of components with other implementations (Figure 1).
These functionalities include basic question-answering using large language models (LLMs) from providers like OpenAI or locally deployed open-source models.
It also includes reproducible prompt engineering to guide LLMs towards specific tasks, knowledge graph (KG) querying with automatic integration of any KG created in the BioCypher framework, retrieval-augmented generation (RAG) using vector database embeddings of user-provided literature, model chaining to orchestrate multiple LLMs and other models in a single conversation using the LangChain framework, fact-checking of LLM responses using a second LLM, and benchmarking of LLMs, prompts, and other components.

In the following, we briefly describe these components, which are demonstrated in our web apps ([https://chat.biocypher.org](https://chat.biocypher.org)).

<!-- Figure 2 -->
![
**The BioChatter framework architecture.**
A) The BioChatter framework components (blue) connect to knowledge graphs and vector databases (orange).
Users (green) can interact with the framework via its Python API, via the lightweight Python frontend using Streamlit (BioChatter Light), or via a fully featured web app with client-server architecture (BioChatter Next).
Developers can write simple frontends using the Streamlit framework, or integrate the REST API provided by the BioChatter Server into their own bespoke solutions.
B) Different use cases of BioChatter on a spectrum of tradeoff between simplicity/economy (left) and security (right). 
Economical and simple solutions involve proprietary services that can be used with little effort but are subject to data privacy concerns.
Increasingly secure solutions require more effort to set up and maintain, but allow the user to retain more control over their data.
Fully local solutions are available given sufficient hardware (starting with contemporary laptops), but are not scalable.
](images/biochatter_architecture.png "Architecture"){#fig:architecture}

### Question Answering and LLM Connectivity

The BioChatter framework interacts with various large language models (LLMs), including proprietary models like the GPT series from OpenAI, as well as open-source models like LLaMA2 and Mixtral 8x7B.
This interaction is facilitated through a flexible open-source deployment framework.
To address data privacy concerns surrounding platforms like ChatGPT, we provide access to different OpenAI models through their API, which has more stringent data protection measures compared to the web interface.
Our preference for open-source LLMs aims to increase transparency and data privacy by enabling local model execution on dedicated hardware and end-user devices.
By leveraging LangChain, we support multiple LLM providers, such as Xorbits Inference and Hugging Face APIs, allowing users to query over 100,000 open-source models on Hugging Face Hub, including those on the LLM leaderboard.
While OpenAI models currently lead in performance and API convenience, we anticipate future open-source developments in this field.
To enhance biomedical AI readiness, we enable plug-and-play model exchange and implement a customized benchmarking framework for the biomedical application of LLMs.

### Prompt Engineering

LLMs are sensitive to prompts, the initial input guiding their behavior.
Prompt engineering lacks established best practices, often relying on manual trial and error [@doi:10.48550/arXiv.2302.11382;@doi:10.48550/arXiv.2312.16171;@biollmbench].
To address this, BioChatter includes a prompt engineering framework for preserving and sharing task-specific prompt sets.
This integration allows for automated evaluation of prompt sets as new models are released, aiding in scalability.

### Knowledge Graphs

Knowledge graphs (KGs) are a useful way to organize and search for information.
We have created BioCypher, a system that builds KGs from biomedical data with a focus on user-friendliness and ontology integration.
BioChatter is an extension of BioCypher that allows users to interact with the data using natural language.
By leveraging information from BioCypher KGs during BioChatter's setup, we improve the efficiency of querying using large language models (LLMs) (refer to Methods).
Connecting BioChatter to any BioCypher KG enables the incorporation of existing knowledge into LLM-based retrieval, enhancing the model's responses through in-context learning.
This approach supports human-AI interaction by grounding responses in KG context.
For a demonstration of KG-driven interaction, refer to Supplementary Note 1 and visit our website at https://biochatter.org/vignette-kg/.

### Retrieval-Augmented Generation

LLM confabulation poses a significant challenge in biomedicine due to the potential consequences of incorrect information.
One approach to mitigating this issue is through "retrieval-augmented generation" (RAG).
RAG involves injecting information into the model prompt of a pre-trained model, eliminating the need for retraining.
This method allows any RAG prompt to be used with any LLM.
While structured knowledge from knowledge graphs (KGs) can be utilized, it is often more efficient to use a semantic search engine to extract information from unstructured data sources like literature.
In the BioChatter framework, we enable users to connect to a vector database, embed multiple documents, and enhance model prompts by incorporating relevant text fragments using semantic search.
The user experience of RAG is demonstrated in Supplementary Note 2 and on our website at https://biochatter.org/vignette-rag/.

### Model Chaining and Fact Checking

Large language models (LLMs) can interact with humans, other LLMs, and various models, understanding API calls for complex tasks [@doi:10.48550/arXiv.2305.15334;@doi:10.48550/arXiv.2308.11432].
Implementing this interaction is challenging and can result in unpredictable behaviors.
To enhance model chaining stability in biomedicine, we develop tailored approaches for tasks like experiment interpretation, literature evaluation, and web exploration.
While leveraging open-source frameworks like LangChain [@langchain], we create custom solutions as needed for application stability.
For instance, we introduce a fact-checking module that employs a second LLM to verify the accuracy of the primary LLM's responses in real-time (see Methods, Figure 1).

### Benchmarking

The broad capabilities of LLMs present challenges for their thorough evaluation, particularly in assisting with various tasks and providing answers in different formats.
To address this, we focus on specific biomedical tasks and datasets and use a second LLM for automated validation of the model's responses.
To ensure transparent and reproducible evaluation, we have developed a benchmarking framework that enables comparison of models, prompt sets, and other pipeline components.
The Pytest framework allows for automated evaluation of all possible component combinations.
Results are stored and displayed on our website for easy comparison, and the benchmark is regularly updated with new models, extensions to datasets, and BioChatter capabilities (https://biochatter.org/benchmark/).

Given the unique tasks and requirements of the biomedical field, we developed a specialized benchmark to evaluate components more accurately [@biollmbench].
This complements existing general benchmarks and leaderboards for Large Language Models (LLMs) [@doi:10.1038/s41586-023-06291-2;@{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard};@{https://crfm.stanford.edu/helm/lite/latest/}].
To address the issue of data leakage from benchmarks to model training data, a common problem in general benchmarks [@doi:10.48550/arXiv.2310.18018], we established an encrypted pipeline that contains benchmark datasets accessible only to the benchmark execution workflow (refer to Methods).

Analysis of the benchmark results (Figure @fig:benchmark A) confirmed OpenAI's strong performance in large language models (LLMs).
The benchmark datasets were tailored to BioChatter's needs, making these results particularly relevant.
OpenAI's GPT models (gpt-4 and gpt-3.5-turbo) outperformed others overall, with gpt-3.5-turbo version 0125 surpassing gpt-4 version 0613.
Interestingly, gpt-4 version 0125 showed a decrease in performance.
Open-source models performed well in specific tasks, with performance leveling off or declining after 4-5 bit quantization.
Model size did not correlate with performance (Pearson's r = 0.171, p < 0.001).

To assess the impact of BioChatter functionality, we compared model performance with and without the prompt engine for KG querying.
Models lacking the prompt engine can access the BioCypher schema definition outlining the KG structure but lack the multi-step procedure offered by BioChatter.
Models without the prompt engine exhibit lower query accuracy compared to those with the prompt engine (0.444±0.11 vs.
0.818±0.11, unpaired t-test P < 0.001, Figure 2B).

<!-- Figure 3 -->
![
**Benchmark results.**
A) Performance of different LLMs (indicated by colour) on the BioChatter benchmark datasets; the y-axis value indicates the average performance across all tasks for each model/size.
X-axis jittered for better visibility.
While the closed-source models from OpenAI mostly show highest performance, open-source models can perform comparably, but show high variance.
Measured performance does not seem to correlate with size (indicated by point size) and quantisation (bit-precision) of the models.
*: Of note, many characteristics of OpenAI models are not public, and thus their bit-precision (as well as the exact size of gpt-4) is subject to speculation.
B) Comparison of the two benchmark tasks for KG querying show the superior performance of BioChatter's prompt engine (0.818±0.11 vs. 0.444±0.11, unpaired t-test P < 0.001).
The test includes all models, sizes, and quantisation levels, and the performance is measured as the average of the two tasks.
The BioChatter variant involves a multi-step procedure of constructing the query, while the "naive" version only receives the complete schema definition of the BioCypher KG (which BioChatter also uses as a basis for the prompt engine).
The general instructions for both variants are the same, otherwise.
](images/biochatter_benchmark.png "Benchmark results"){#fig:benchmark}
