## Results

BioChatter ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) is a Python framework that provides an easy-to-use interface to interact with LLMs and auxiliary technologies via an intuitive API (application programming interface).
This way, its functionality can be integrated into any number of user interfaces, such as web apps, command-line interfaces, or Jupyter notebooks (Figure @fig:architecture).

The framework we developed is designed with flexibility in mind, allowing for the easy exchange of its components with alternative implementations, as depicted in Figure 1.
This modular design supports a wide range of functionalities crucial for the biomedical application of large language models (LLMs), including:

- Basic question-answering capabilities, utilizing LLMs provided by external services like OpenAI, as well as those that are open-source and deployed locally.

- Reproducible prompt engineering, which enables the tailoring of LLM responses for specific tasks or desired behaviors.

- Knowledge graph querying, featuring seamless integration with any knowledge graph created using the BioCypher framework, facilitating direct access to structured biomedical data.

- Retrieval-augmented generation (RAG), which leverages vector database embeddings of user-provided literature to enhance the generation process with relevant information retrieval.

- Model chaining, a process that allows for the orchestration of multiple LLMs and other models within a single conversational thread, using the LangChain framework for efficient model interaction.

- Fact-checking of LLM-generated responses through the use of a secondary LLM, ensuring the reliability and accuracy of the information provided.

- Benchmarking tools for evaluating the performance of LLMs, prompts, and other system components, ensuring optimal operation and results.

Each of these functionalities is integral to the platform's ability to serve as a comprehensive tool for the biomedical application of large language models, offering users a robust and versatile framework for their research and development needs.

In the following, we briefly describe these components, which are demonstrated in our web apps ([https://chat.biocypher.org](https://chat.biocypher.org)).

<!-- Figure 2 -->
![
**The BioChatter framework architecture.**
A) The BioChatter framework components (blue) connect to knowledge graphs and vector databases (orange).
Users (green) can interact with the framework via its Python API, via the lightweight Python frontend using Streamlit (BioChatter Light), or via a fully featured web app with client-server architecture (BioChatter Next).
Developers can write simple frontends using the Streamlit framework, or integrate the REST API provided by the BioChatter Server into their own bespoke solutions.
B) Different use cases of BioChatter on a spectrum of tradeoff between simplicity/economy (left) and security (right). 
Economical and simple solutions involve proprietary services that can be used with little effort but are subject to data privacy concerns.
Increasingly secure solutions require more effort to set up and maintain, but allow the user to retain more control over their data.
Fully local solutions are available given sufficient hardware (starting with contemporary laptops), but are not scalable.
](images/biochatter_architecture.png "Architecture"){#fig:architecture}

### Question Answering and LLM Connectivity

BioChatter is designed to seamlessly integrate with Large Language Models (LLMs), offering support for both well-known proprietary models like OpenAI's GPT series and open-source alternatives such as LLaMA2 and Mixtral 8x7B, through a versatile open-source deployment framework (see Methods section).
Given the growing data privacy concerns surrounding ChatGPT, OpenAI's flagship conversational AI, particularly highlighted by discussions at the European Data Protection Board, we've ensured access to OpenAI models via their API.
This method adheres to stricter data protection standards, notably prohibiting the reuse of user inputs for training future models, thus addressing privacy issues.

We prioritize supporting open-source LLMs to enhance transparency and data privacy.
This is achieved by enabling models to run locally on dedicated hardware or user devices.
Our system is built on LangChain, allowing us to connect with numerous LLM providers, including Xorbits Inference and Hugging Face APIs.
This setup grants access to over 100,000 open-source models available on the Hugging Face Hub, including those leading its LLM leaderboard.

Although OpenAI's models currently lead in performance and API usability, we anticipate significant advancements in open-source models.
To accommodate this evolution and bolster biomedical AI's readiness, BioChatter facilitates easy model swapping and incorporates a custom benchmarking framework specifically for biomedical applications of LLMs.

### Prompt Engineering

An essential aspect of Large Language Models (LLMs) is how they respond to prompts, which are the initial inputs that steer the model towards performing a specific task or behavior.
Prompt engineering is a growing field within practical AI, yet it lacks standardized best practices.
Current methods largely depend on a trial-and-error approach, which is not only non-reproducible but also varies with each new model.
To tackle this challenge, our platform, BioChatter, incorporates a prompt engineering framework designed to store and manage prompt sets for particular tasks.
These sets can then be shared and reused within the community, fostering collaboration and efficiency.
Moreover, we've seamlessly integrated this framework into our benchmarking pipeline.
This integration allows for the automated evaluation of prompt sets against newly published models, streamlining the process of prompt engineering as the field evolves.

### Knowledge Graphs

Knowledge Graphs (KGs) serve as a robust method for organizing and accessing structured knowledge.
Our development, BioCypher, simplifies the creation of KGs from biomedical data, ensuring data is meaningfully connected to relevant ontologies.
Building on BioCypher, BioChatter enhances user experience by enabling interactions with the data through natural language.
This compatibility means any KG created with BioCypher can be seamlessly used with BioChatter.
We improve BioChatter's ability to understand and query KGs by utilizing insights from the construction of BioCypher KGs.
This process not only makes queries more efficient but also allows the Large Language Model (LLM) to incorporate existing knowledge into its responses.
This approach, known as retrieval-augmented generation, enriches the LLM's answers with context from the KG, facilitating more meaningful human-AI interactions.
We showcase the practical benefits of this KG-driven interaction in our supplementary material and on our website at [https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/).

### Retrieval-Augmented Generation

The issue of generating incorrect information, known as confabulation, is particularly critical in biomedical applications due to the potential for harmful consequences.
A recent strategy to mitigate this problem is the use of "retrieval-augmented generation" (RAG), which enhances a pre-trained model's responses by incorporating relevant information directly into the model's prompt without the need for retraining.
This approach can leverage structured knowledge from knowledge graphs (KGs) or, more efficiently, use a semantic search engine to pull pertinent information from vast unstructured data sources like scientific literature.
Our platform, BioChatter, integrates this technique by facilitating connection to a vector database, enabling document embedding, and employing semantic search to refine model prompts with text snippets relevant to the posed questions.
This process, aimed at improving accuracy and reliability, is detailed in our Methods section and further illustrated through a user experience demonstration available in Supplementary Note 2 and on the BioChatter website ([https://biochatter.org/vignette-rag/](https://biochatter.org/vignette-rag/)).

### Model Chaining and Fact Checking

Large Language Models (LLMs) are not limited to interfacing with human users; they can also interact with other LLMs and various model types.
Their ability to understand API calls theoretically enables them to orchestrate complex, multi-step tasks.
However, putting this into practice is challenging, and the process can result in unpredictable behaviors.
Our goal is to enhance the stability of chaining models together in biomedical contexts by creating tailored approaches for specific tasks such as experiment interpretation and design, literature evaluation, and web resource exploration.
Although we primarily utilize existing open-source frameworks like LangChain, we also craft custom solutions when necessary to ensure stability for particular applications.
For instance, we've developed a fact-checking module that employs a secondary LLM to continuously assess the accuracy of the primary LLM's responses during interactions (refer to the Methods section).

### Benchmarking

The widespread applicability of Large Language Models (LLMs) introduces significant challenges for their thorough evaluation.
Their versatility in handling various tasks and the flexibility in how they present answers make traditional evaluation methods less effective.
To address this, our study focuses on specific tasks within biomedicine, utilizing datasets tailored to this field.
We enhance the evaluation process by using a second LLM to automatically validate the responses from the primary model, ensuring a more advanced assessment.

To facilitate a clear and reproducible comparison of LLMs, we have developed a benchmarking framework.
This framework is designed to compare different models, prompt sets, and other pipeline components efficiently.
We leverage the Pytest framework to automate the evaluation across a comprehensive matrix of component combinations.
The outcomes of these evaluations are readily accessible on our website, allowing for easy comparison.
Furthermore, this benchmark is regularly updated to reflect the introduction of new models, dataset extensions, and improvements in the BioChatter platform.
For more details, visit our benchmark page at [https://biochatter.org/benchmark/](https://biochatter.org/benchmark/).

In the biomedical field, specific tasks and requirements necessitate tailored evaluation tools.
To address this, we developed a custom benchmark designed for precise assessment of components within the biomedicine domain.
This benchmark serves as a complement to existing, general-purpose benchmarks and leaderboards for large language models (LLMs), such as those found in recent publications and online platforms.
Additionally, to avoid the risk of our benchmark data contaminating the training datasets of the models—a common problem with general-purpose benchmarks—we established an encrypted pipeline.
This pipeline securely houses the benchmark datasets and is exclusively accessible to the system that runs the benchmark evaluations (refer to the Methods section for more details).

Our analysis of benchmark datasets, designed to assess functions critical to BioChatter's biomedical applications, affirmed OpenAI's dominance in large language model (LLM) performance, as shown in Figure @fig:benchmark A.
This analysis underscores the relevance of LLMs like OpenAI's GPT models (gpt-4 and gpt-3.5-turbo) to our platform, with these models leading in both overall performance and consistency.
However, it's noteworthy that certain open-source models exhibit high performance in specific tasks.
Interestingly, the newer gpt-3.5-turbo (version 0125) surpasses the previous gpt-4 version (0613) in performance, while the latest gpt-4 version (0125) experiences a significant performance decline.

The analysis also reveals that the performance of open-source models is influenced by their quantization level, which refers to the precision of the model's parameters.
For models offering quantization options, we observed that performance tends to plateau or even decrease beyond 4 or 5 bits, as detailed in Figure @fig:benchmark A.
Additionally, our findings indicate no significant correlation between the size of a model and its performance, with a Pearson's correlation coefficient (r) of 0.171 and a p-value of less than 0.001.
This suggests that factors other than size play a critical role in determining a model's effectiveness in biomedical applications.

To assess the effectiveness of BioChatter, we compared the performance of models utilizing BioChatter's prompt engine for Knowledge Graph (KG) querying against those without it.
Although models lacking the prompt engine could access the BioCypher schema—which outlines the KG structure—they did not employ BioChatter's advanced querying process.
This resulted in a significant difference in their ability to generate accurate queries.
Models using the prompt engine outperformed those without, achieving a higher accuracy (0.818±0.11) compared to the latter's 0.444±0.11, as confirmed by an unpaired t-test (P < 0.001, Figure @fig:benchmark B).

<!-- Figure 3 -->
![
**Benchmark results.**
A) Performance of different LLMs (indicated by colour) on the BioChatter benchmark datasets; the y-axis value indicates the average performance across all tasks for each model/size.
X-axis jittered for better visibility.
While the closed-source models from OpenAI mostly show highest performance, open-source models can perform comparably, but show high variance.
Measured performance does not seem to correlate with size (indicated by point size) and quantisation (bit-precision) of the models.
*: Of note, many characteristics of OpenAI models are not public, and thus their bit-precision (as well as the exact size of gpt-4) is subject to speculation.
B) Comparison of the two benchmark tasks for KG querying show the superior performance of BioChatter's prompt engine (0.818±0.11 vs. 0.444±0.11, unpaired t-test P < 0.001).
The test includes all models, sizes, and quantisation levels, and the performance is measured as the average of the two tasks.
The BioChatter variant involves a multi-step procedure of constructing the query, while the "naive" version only receives the complete schema definition of the BioCypher KG (which BioChatter also uses as a basis for the prompt engine).
The general instructions for both variants are the same, otherwise.
](images/biochatter_benchmark.png "Benchmark results"){#fig:benchmark}
