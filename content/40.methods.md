## (Supplementary / Online) Methods {.page_break_before}

BioChatter (version 0.4.7 at the time of writing) is a Python library designed for the biomedical application of large language models, specifically catering to Python versions 3.10 through 3.12.
This compatibility is ensured through a rigorous continuous integration pipeline hosted on GitHub ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)).
Comprehensive documentation, including a step-by-step tutorial and a detailed API reference, is made available at [https://biochatter.org](https://biochatter.org) to facilitate ease of use and to promote widespread adoption.
The development of all packages is conducted openly, aligning with the modern standards of software development as advocated by [@doi:10.1038/s41597-020-0486-7].
To foster an environment conducive to collaboration and innovation, we have adopted the permissive MIT license, which encourages both downstream use and further development by the community.
Additionally, to ensure that our platform is accessible and inclusive to a broad audience interested in contributing to the framework, we have implemented a code of conduct along with comprehensive contributor guidelines.

### Applications

To demonstrate basic and advanced use cases of the framework, we provide two web apps, BioChatter Light and BioChatter Next.

BioChatter Light is a web application developed using the Streamlit framework (version 1.31.1) ([Streamlit, 2021](https://streamlit.io)), characterized by its Python-based coding environment, allowing for both local and server deployment ([BioChatter Light GitHub Repository](https://github.com/biocypher/biochatter-light)).
Streamlit's design philosophy prioritizes the ease of creating interactive web applications using pure Python, facilitating quick iterations and the agile development of novel functionalities.
However, this comes at the cost of reduced customization options and scalability potential.
Such a framework is particularly advantageous for the swift prototyping of tailored solutions aimed at addressing specific use cases.
For the most recent updates and a demonstration of the platform's capabilities, interested parties are encouraged to visit the [online preview](https://chat.biocypher.org).

BioChatter Next ([https://github.com/biocypher/biochatter-next](https://github.com/biocypher/biochatter-next)) represents an advanced web application employing a server-client architecture, inspired by the open-source framework of ChatGPT-Next-Web ([https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web)).
The application is developed using a combination of Typescript and Python, leveraging Next.js (v13.4.9) for crafting a responsive frontend while utilizing Flask (v3.0.0) for robust backend services.
This setup showcases the integration capabilities of BioChatter within a contemporary web environment, offering extensive customization options, scalability features, and support for localization in 18 different languages.
Despite these advantages, the implementation's complexity and the prolonged development timeline present notable challenges.
To facilitate the straightforward incorporation of the BioChatter backend with pre-existing frontend infrastructures, we have made available the server-side implementation via [https://github.com/biocypher/biochatter-server](https://github.com/biocypher/biochatter-server) and also as a Docker image within our Docker Hub organization ([https://hub.docker.com/repository/docker/biocypher/biochatter-server](https://hub.docker.com/repository/docker/biocypher/biochatter-server)).
This approach enables developers to leverage the BioChatter technology efficiently, ensuring a seamless integration process.

We invite all interested researchers to select the framework that best suits their needs, or use the BioChatter server or library in their existing solutions.

### Benchmarking

The benchmarking framework we developed systematically evaluates a matrix of different component combinations by leveraging the parameterization feature of Pytest [@pytest].
This approach facilitates the automated assessment of all conceivable combinations of components, including Large Language Models (LLMs), prompts, and datasets.
The benchmarking process was conducted on a MacBook Pro equipped with an M3 Max chip, featuring a 40-core GPU and 128GB of RAM.
By default, to mitigate the inherent stochastic variability of LLMs, we executed each test five times.
Typically, we adjusted the temperature parameter to the lowest feasible value for each model to minimize fluctuations in the results.

The Pytest matrix employs a hash-based methodology to determine the necessity of executing a specific model-dataset combination test.
To elaborate, the hash is generated from the dictionary representation of the test parameters.
If the database already contains an entry matching the combination of this hash and the model name, the corresponding test is omitted.
This approach enhances efficiency by ensuring that only those tests which involve modified or newly introduced parameters are executed.
The dimensions considered within this matrix include:

- **LLMs**: The framework aims to benchmark both proprietary models, such as those developed by OpenAI, and open-source models, which are frequently accessed via the Xorbits Inference API or through HuggingFace's suite of models.
A key feature of our framework is its capability to automate the deployment and testing of these open-source models in a consistent manner.

- **Prompts**: Given the significant impact of prompt design on model performance, our methodology involves the use of a diverse array of prompts for each task.
These prompts vary in specificity and include both fixed and adaptable components to thoroughly evaluate performance variability.

- **Datasets**: A variety of datasets, formatted in a question-answer style, are utilized to test the models across different tasks.
This approach allows for a comprehensive assessment of model capabilities.

- **Data Processing**: The preprocessing steps can markedly influence the performance of LLMs.
For example, the framework tests the efficacy of converting numerical data into categorical text representations (e.g., converting numerical values into 'low', 'medium', 'high' categories) since LLMs typically struggle with processing raw numerical data.

- **Model Quantisations**: The framework examines different quantisation settings for each model, where applicable.
This evaluation helps in understanding the balance between model size, inference speed, and overall performance.

- **Model Parameters**: The framework also explores various model parameters, such as "temperature," which affects the consistency of model responses.
Testing different settings allows for optimization of model outputs.

- **Integrations**: Specialized tests are designed for tasks that necessitate integration with external systems, such as knowledge graphs or vector databases.
This ensures that the models can effectively interact with and leverage these systems.

- **Stochasticity**: To accommodate the inherent variability in model responses, the framework includes an option to execute each test multiple times.
This facilitates the generation of summary statistics that provide a more robust understanding of model performance.

- **Sentiment and Behaviour**: The framework assesses the models' adherence to desired behavioral patterns by employing a second LLM to evaluate the responses.
This evaluation is based on criteria such as professionalism and politeness, ensuring that the models align with the expected standards for each persona.

The Pytest framework is deployed for the comprehensive evaluation of our platform, accessible at [https://github.com/biocypher/biochatter/blob/main/benchmark](https://github.com/biocypher/biochatter/blob/main/benchmark), with detailed benchmarking information provided at [https://biochatter.org/benchmarking](https://biochatter.org/benchmarking).
This benchmark is dynamically updated following the introduction of novel models and dataset expansions, maintaining its availability at [https://biochatter.org/benchmark](https://biochatter.org/benchmark).
Upon community requests, which can be submitted through our GitHub issue template at [https://github.com/biocypher/biochatter/issues/new/choose](https://github.com/biocypher/biochatter/issues/new/choose), we engage in benchmarking new models and their variants, including those that have been fine-tuned. 

This approach to maintaining a "living benchmark" is derived from principles of test-driven development, wherein test cases are formulated to reflect specific desired features or behaviors.
It is common for a model to not achieve the optimal response initially, necessitating iterative adjustments to the framework's components, such as prompts or functions, to improve the model's performance.
By continuously monitoring how the model fares against these test scenarios over time, we are able to evaluate the framework's consistency and identify areas that warrant further refinement.

To mitigate the risk of benchmarking data leakage, which could lead to the contamination of future Large Language Models (LLMs), we have introduced an encryption routine for the benchmark datasets.
This encryption employs a hybrid scheme, combining both symmetric and asymmetric encryption methods.
Initially, the data are encrypted using a symmetric key; this key is then encrypted with an asymmetric key for added security.
Ensuring the confidentiality and integrity of the datasets, they are stored within a specially designed encrypted pipeline.
This pipeline is exclusively accessible by the workflow designated for executing the benchmark tests, preventing unauthorized access and ensuring data security.

The implementation details of these processes are made available at the GitHub repository [https://github.com/biocypher/llm-test-dataset](https://github.com/biocypher/llm-test-dataset).
This repository serves as a resource for those interested in understanding the technical specifics of the encryption routine and its integration into the benchmark procedure within the BioChatter platform.
By adopting this approach, we ensure the robust protection of sensitive benchmarking data against potential threats, thereby preserving the integrity of future research and development efforts in the biomedical application of large language models.

### Knowledge Graphs

We leverage the synergistic relationship between BioChatter and the BioCypher framework [@biocypher] to seamlessly incorporate knowledge graph (KG) queries within the BioChatter Application Programming Interface (API).
In the process of constructing the BioCypher KG, a configuration file plays a pivotal role in aligning KG contents with ontology terms.
This involves specifying details about the entities encompassed within the KG.
For example, we elucidate the attributes of a node and define the source and target categories of an edge.
Moreover, throughout the KG construction phase, we augment this base of information and store it in a YAML file.
Optionally, this enriched data can be directly integrated into the KG.
Such detailed information facilitates BioChatter in refining its comprehension of the KG, thereby enhancing the Large Language Model's (LLM) ability to conduct queries against the KG more effectively.

To facilitate the Large Language Model's (LLM) generation of accurate queries, it is crucial to have a comprehensive understanding of the Knowledge Graph (KG) context, including the precise contents, identifiers, and property spellings.
BioChatter addresses this by dividing the query generation process into distinct stages: identifying entities and relationships from the user's query, determining the properties to be included in the query, and formulating a syntactically correct query in the database's query language.
These steps are informed by the prior analysis of the KG schema and the outcomes of the preceding stages.
This multi-step process is encapsulated within the `prompts.py` module.
In order to assess the efficacy of this approach, a specific module within our benchmark suite is dedicated to evaluating the query generation process.
This evaluation employs a variety of questions and KG schemata to rigorously test the system's performance.

To demonstrate the practical application of our platform, we have established a demonstration repository, which is accessible at [https://github.com/biocypher/pole](https://github.com/biocypher/pole).
This repository includes a detailed procedure for constructing a Knowledge Graph (KG) and features an instance of BioChatter Light.
This instance can be easily deployed using a single command through Docker Compose.
Furthermore, the constructed pole KG can be seamlessly integrated with the BioChatter Next application.
This integration is facilitated by utilizing the provided `docker-compose.yaml` file, which enables users to build the application locally.
For a comprehensive demonstration of this use case, interested readers are directed to [Supplementary Note 1: Knowledge Graph Retrieval-Augmented Generation] and our dedicated website at [https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/).
This approach underscores the versatility and user-friendliness of our platform, making it accessible for a wide range of biomedical applications.

### Retrieval-Augmented Generation

While current Large Language Models (LLMs) contain a broad internal general knowledge base, they may not effectively prioritize specific scientific findings or may lack access to certain research articles in their training data, possibly due to the articles' recent publication dates or licensing restrictions.
To address this limitation, we propose supplementing the model with additional information from pertinent publications through the prompt mechanism.
However, the inherent limitation regarding the input length of contemporary models restricts our ability to include entire publications within the prompt.
Consequently, it becomes imperative to distill the information specifically relevant to the user's query.
To achieve this, we employ a method of semantic similarity search, comparing the user's question against the content of user-provided scientific articles or other textual sources.
The most effective approach for this comparison involves the utilization of a vector database, as detailed in [@doi:10.48550/arxiv.2308.07107].
This methodology allows for the efficient mapping of the question to the most relevant information contained within the provided texts.

In the proposed framework, the initial step involves decomposing the contextual background information, provided by the user, into manageable segments that are amenable to processing by large language models (LLMs).
Each segment is then transformed into a high-dimensional vector through an embedding process.
These vectors, which represent the embedded text fragments, are subsequently stored in a specialized vector database.
The primary advantage of this approach lies in the ability to perform rapid and efficient retrieval of text fragments based on the similarity of their vector representations.
This method facilitates the identification of closely related entities by comparing the vectors associated with each text fragment.
For instance, consider the sentences "Amyloid beta levels are associated with Alzheimer’s Disease stage." and "One of the most important clinical markers of AD progression is the amount of deposited A-beta 42." Despite their different wording, these sentences convey similar concepts and would thus be identified as closely related within the vector database, assuming the embedding model employed is of high quality, akin to or surpassing that of GPT-3.
This contrasts with traditional text-based similarity metrics, which may not recognize the high degree of similarity between these sentences due to their differing lexical compositions.

To enhance the process of extracting pertinent information from an extensive knowledge base, our approach involves comparing the user's query against a pre-existing vector database to identify relevant data segments.
This methodology can be further optimized by initially employing a Large Language Model (LLM) to formulate a preliminary response to the user's inquiry.
Subsequently, this generated response, regardless of its accuracy, is utilized to conduct a search within the vector database for pertinent information.
This strategy is predicated on the hypothesis that the generated "fake answer" is more likely to exhibit semantic alignment with the information relevant to the user's query than the query itself, thereby improving the efficiency of the retrieval process [@doi:10.48550/arXiv.2308.07107].
The outcome of this semantic search, typically comprising concise sentences directly related to the query topic, can be integrated into the model's prompt.
This integration facilitates the model's ability to assimilate new context without necessitating retraining or fine-tuning, a process often referred to as in-context learning [@doi:10.48550/arxiv.2303.17580] or retrieval-augmented generation [@rag].
This approach capitalizes on the model's inherent capability to adapt and learn from additional contextual information, thereby enhancing its performance and applicability in biomedical applications.

To facilitate access to the advanced functionalities within BioChatter, we have developed specific classes for establishing and managing connections to vector database systems, encapsulated within the `vectorstore.py` module.
Additionally, the `vectorstore_agent.py` module is tasked with conducting semantic searches within the vector database and integrating the search outcomes into the input prompts.
A similar approach is employed for knowledge graph (KG) retrieval, managed by the `database_agent.py` module.
These retrieval processes are seamlessly integrated and made accessible through the BioChatter API via the `rag_agent.py` module. 

To illustrate the practical application of this API, we introduced a "Retrieval-Augmented Generation" feature in the preview apps.
This feature enables users to upload text documents into a vector database.
Subsequently, these documents can be queried to enrich the prompts sent to the primary model with contextual information, which is clearly displayed to the user.
Given that this feature necessitates a connection to a vector database system, we ensure connectivity to a Milvus service.
This includes providing a streamlined method for initiating the service alongside a BioCypher knowledge graph and the BioChatter Light application within a single Docker Compose workflow. 

This comprehensive approach not only enhances the usability of BioChatter but also leverages the power of retrieval-augmented generation and knowledge graphs to enrich the input data, thereby potentially improving the performance and relevance of the generated outputs in biomedical applications.

An example use case of this functionality is available in [Supplementary Note 2: Retrieval-Augmented Generation] and on our website ([https://biochatter.org/vignette-rag/](https://biochatter.org/vignette-rag/)).

### Deployment of Open-Source Models

To ensure broad accessibility to cutting-edge open-source models, our approach incorporates a versatile deployment framework via the Xorbits Inference API [@{https://github.com/xorbitsai/inference}].
This framework readily supports a comprehensive array of open-source models by default.
Moreover, it facilitates the seamless integration of additional models from the Hugging Face Hub [@{https://huggingface.co/}] through an easy-to-use graphical user interface.
For the deployment of the models evaluated in this study, we utilized version 0.8.4 of Xorbits Inference.
To aid in the deployment process on a Linux server equipped with Nvidia GPUs, we have made available a Docker Compose repository ([https://github.com/biocypher/xinference-docker-builtin/](https://github.com/biocypher/xinference-docker-builtin/)).
This particular Docker Compose setup leverages a multi-architecture image compatible with both ARM64 and AMD64 processors, which is accessible through our Docker Hub organization ([https://hub.docker.com/repository/docker/biocypher/xinference-builtin](https://hub.docker.com/repository/docker/biocypher/xinference-builtin)).
It is important to note that, in the context of Mac OS systems with Apple Silicon chips, Docker is unable to interface with the GPU driver.
Consequently, in such scenarios, Xorbits Inference requires native deployment to function properly.

### Model Chaining

The capability of large language models (LLMs) to manage external software, including other LLMs, introduces a plethora of opportunities for managing intricate tasks.
A straightforward illustration of this is the establishment of a correcting agent.
This agent receives the output from the primary model and evaluates it for factual accuracy.
Should the agent identify inaccuracies, it possesses the ability to either prompt the primary model for correction or directly communicate the necessary adjustments to the user.
This process fundamentally relies on the correcting agent's internal knowledge base, thereby subjecting it to similar limitations; namely, the potential for the correcting agent to generate confabulated information exists.
However, given that the correcting agent operates independently from the primary model—being configured with specific prompts—it is less prone to replicate the same confabulation errors as the primary model.

This methodological approach is rooted in the retrieval-augmented generation framework, which has been demonstrated to enhance the factual accuracy of LLM outputs by incorporating external databases or knowledge graphs into the generation process (Lewis et al., 2020).
The correcting agent can be conceptualized as an application of this framework, wherein it serves as an intermediary layer that assesses and refines the output of the primary LLM based on a dedicated knowledge source.

The effectiveness of such a correcting agent is contingent upon its ability to accurately identify and rectify errors in the primary model's output.
This involves complex decision-making processes, where the agent must determine the veracity of the primary model's output, identify the nature of any detected inaccuracies, and generate appropriate corrections.
The performance of the correcting agent, therefore, hinges on its training data, the comprehensiveness of its knowledge base, and the sophistication of its error-detection algorithms.

In conclusion, the integration of a correcting agent represents a promising avenue for enhancing the reliability

This approach can be extended to a more complex model chain.
For instance, the correcting agent could query a knowledge graph or a vector database to base its responses on prior knowledge.
Implementing these chains is straightforward, and some configurations are readily available within the LangChain framework [@langchain].
However, their behavior can become unpredictable, a tendency that grows with each additional link in the chain.
As such, close monitoring and control are essential.
Additionally, these chains increase the computational load on the system, a factor that becomes particularly significant when deploying on end-user devices.
