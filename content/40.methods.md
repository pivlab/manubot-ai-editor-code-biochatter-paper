## (Supplementary / Online) Methods {.page_break_before}

BioChatter (version 0.4.7 at the time of publication) is a Python library that supports Python 3.10-3.12.
The compatibility with these versions is ensured through a continuous integration pipeline on GitHub (https://github.com/biocypher/biochatter).
Documentation for BioChatter can be found at https://biochatter.org, which includes a tutorial and API reference.
The development of all packages is done openly and follows modern software development standards [@doi:10.1038/s41597-020-0486-7].
To encourage downstream use and development, we have chosen the permissive MIT license.
Additionally, we have included a code of conduct and contributor guidelines to promote accessibility and inclusivity for all individuals interested in contributing to the framework.

$$
\text{Equation (@id): Definition of important symbol}
$$

### Applications

To demonstrate basic and advanced use cases of the framework, we provide two web apps, BioChatter Light and BioChatter Next.

BioChatter Light is a web app based on the Streamlit framework (version 1.31.1, [https://streamlit.io](https://streamlit.io)), which is written in Python and can be deployed locally or on a server ([https://github.com/biocypher/biochatter-light](https://github.com/biocypher/biochatter-light)).
The ease with which Streamlit allows the creation of interactive web apps in pure Python enables rapid iteration and agile development of new features, with the tradeoff of limited customization and scalability.
This framework is suitable for rapid prototyping of bespoke solutions for specific use cases.
For an up-to-date overview and preview of the current functionality of the platform, please visit the [online preview](https://chat.biocypher.org).

BioChatter Next ([https://github.com/biocypher/biochatter-next](https://github.com/biocypher/biochatter-next)) is a contemporary web application with a server-client architecture, built upon the open-source framework of ChatGPT-Next-Web ([https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web)).
It is developed using a combination of Typescript and Python, utilizing Next.js (v13.4.9) for the frontend and Flask (v3.0.0) for the backend.
The platform showcases the application of BioChatter within a modern web environment, offering extensive customization, scalability, and support for localization in 18 languages.
Nevertheless, this enhanced functionality is accompanied by increased complexity and longer development cycles.
To facilitate the seamless integration of the BioChatter backend with existing frontend solutions, we offer the server implementation at [https://github.com/biocypher/biochatter-server](https://github.com/biocypher/biochatter-server), as well as a Docker image within our Docker Hub organization ([https://hub.docker.com/repository/docker/biocypher/biochatter-server](https://hub.docker.com/repository/docker/biocypher/biochatter-server)).

We invite all interested researchers to select the framework that best suits their needs, or use the BioChatter server or library in their existing solutions.

### Benchmarking

The benchmarking framework examines a matrix of component combinations using the parameterization feature of Pytest [@pytest].
This implementation allows for the automated evaluation of all possible combinations of components, such as Large Language Models (LLMs), prompts, and datasets.
We performed the benchmarks on a MacBook Pro with an M3 Max chip with a 40-core GPU and 128GB of RAM.
As a default, we ran each test five times to account for the stochastic nature of LLMs.
We generally set the temperature to the lowest value possible for each model to decrease fluctuation.

$$
\text{Symbols:}\\
\text{LLMs} - \text{Large Language Models}
$$

The Pytest matrix utilizes a hash-based system to determine if a model-dataset combination has been previously executed.
In essence, the hash is computed from the dictionary representation of the test parameters, and the test is skipped if the hash combined with the model name already exists in the database.
This hashing strategy optimizes efficiency by only executing modified or newly added tests.
The key dimensions of the matrix include:

- **LLMs**: The primary objective of our benchmarking framework is to test both proprietary (OpenAI) and open-source models (commonly leveraging the Xorbits Inference API and HuggingFace models) against the same set of tasks.
We streamline the testing process by incorporating a programmatic approach to deploying open-source models.

- **prompts**: Given that model performance can significantly depend on the prompts used, we employ a range of prompts for each task with varying levels of specificity and components that are fixed or variable to assess this variability.

- **datasets**: Various tasks are tested using specific datasets structured in a question-answer format.

- **data processing**: Certain data processing steps can significantly impact the subsequent performance of LLMs.
For example, we evaluate the conversion of numbers (which LLMs often struggle with) into categorical text (e.g., low, medium, high).

- **model quantizations**: We assess a variety of quantizations for each model (where applicable) to consider the trade-off between model size, inference speed, and performance.

- **model parameters**: When appropriate, we examine a range of parameters for each model, such as "temperature," which influences the reproducibility of model outputs.

- **integrations**: Dedicated tests are developed for specific tasks that necessitate integrations, such as with knowledge graphs or vector databases.

- **stochasticity**: To address the variability in model responses, we incorporate a parameter to run each test multiple times and generate summary statistics.

- **sentiment and behavior**: To evaluate whether the models demonstrate the desired behavior patterns for different personas, we employ a second LLM to assess the responses based on criteria like professionalism and politeness.

The Pytest framework is implemented at [https://github.com/biocypher/biochatter/blob/main/benchmark](https://github.com/biocypher/biochatter/blob/main/benchmark), and more information is available at [https://biochatter.org/benchmarking](https://biochatter.org/benchmarking).
The benchmark is updated upon the release of new models and extensions to the datasets, and continuously available at [https://biochatter.org/benchmark](https://biochatter.org/benchmark).

We will run the benchmark on new models and variants (including fine-tuned models) upon requests from the community, which can be made on GitHub using our issue template ([https://github.com/biocypher/biochatter/issues/new/choose](https://github.com/biocypher/biochatter/issues/new/choose)).
The living benchmark process is inspired by test-driven development, meaning test cases are created based on specific features or behaviors that are desired.
When a model doesn't initially produce the optimal response, which is often the case, adjustments are made to various elements of the framework, including prompts or functions, to enhance the model's effectiveness.

Monitoring the model's performance on these tests over time allows us to assess the framework's reliability and pinpoint areas that need improvement.

To prevent leakage of benchmarking data (and subsequent contamination of future LLMs), an encryption routine is implemented on the benchmark datasets.
The encryption is carried out using a hybrid encryption scheme, where the data are encrypted with a symmetric key, which is then encrypted with an asymmetric key.
The datasets are stored in a dedicated encrypted pipeline that is exclusively accessible to the workflow executing the benchmark.
These processes are implemented at [https://github.com/biocypher/llm-test-dataset](https://github.com/biocypher/llm-test-dataset) and accessed from the benchmark procedure in BioChatter.

### Knowledge Graphs

We utilize the close connection between BioChatter and the BioCypher framework (Johnson et al., 2020) to integrate knowledge graph (KG) queries into the BioChatter API.
In the BioCypher KG creation, we use a configuration file to map KG contents to ontology terms, including information about each of the entities.
For instance, we detail the properties of a node and the source and target classes of an edge.
Additionally, during the KG build process, we enrich this information and save it to a YAML file and, optionally, directly to the KG.
This information is used by BioChatter to tune its understanding of the KG, which allows the Large Language Model (LLM) to query the KG more efficiently.

$$
\text{BioCypher KG creation process:}
$$ {#kg_creation}

The BioCypher KG creation process involves using a configuration file to map KG contents to ontology terms.
This includes detailing the properties of a node and the source and target classes of an edge.
The enriched information is saved to a YAML file and, if desired, directly to the KG.
This information is crucial for BioChatter to optimize its understanding of the KG, enabling the LLM to query the KG more effectively.

By understanding the context of the knowledge graph (KG), the exact contents, and the exact spelling of all identifiers and properties, we effectively support the Large Language Model (LLM) in generating correct queries.
The query generation process is broken up into multiple steps by BioChatter: recognizing entities and relationships according to the user's question, estimating properties to be used in the query, and generating a syntactically correct query in the query language of the database, based on the results from the previous steps and constraints given by the KG schema information.
This procedure is implemented in the `prompts.py` module.

$$
\text{To evaluate the quality of this process, we dedicate a module in the benchmark to the query generation process with a range of questions and KG schemata.}
$$

This evaluation process helps us understand how well the LLM can generate queries based on the information provided by the KG and the user's input.
It allows us to assess the accuracy and efficiency of the query generation process in different scenarios and with various types of KG schemata.

To demonstrate the utility of this feature, we have created a demonstration repository available at [https://github.com/biocypher/pole](https://github.com/biocypher/pole).
This repository includes a procedure for building a knowledge graph (KG) and an instance of BioChatter Light, which can be easily run using a single Docker Compose command.
Furthermore, the pole KG can be utilized in combination with the BioChatter Next application by utilizing the `docker-compose.yaml` file to locally build the application.
A demonstration showcasing this use case can be found in [Supplementary Note 1: Knowledge Graph Retrieval-Augmented Generation] and is also accessible on our website at [https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/).

### Retrieval-Augmented Generation

While current Large Language Models (LLMs) possess extensive internal general knowledge, they may not prioritize very specific scientific results or have access to all research articles in their training data, possibly due to recency or licensing issues.
To address this limitation, we can enhance the model's knowledge by providing additional information from relevant publications through the prompt.
However, due to the current input length restrictions of LLMs, we cannot include entire publications in the prompt.
Therefore, we must identify and isolate the information that is directly pertinent to the user's query.
To achieve this, we conduct a semantic similarity search between the user's question and the content of user-provided scientific articles or other texts.
Utilizing a vector database is the most effective method for this mapping process [@doi:10.48550/arxiv.2308.07107].

$$
LLM: Large Language Model
$$

The contextual background information provided by the user, such as a scientific article related to the experiment to be interpreted, is divided into segments suitable for processing by the Large Language Model (LLM).
Each segment is individually embedded by the model.
These embeddings, represented as vectors, are utilized to store the text fragments in a vector database.
Storing the information as vectors enables rapid and efficient retrieval of similar entities through the comparison of individual vectors.
For instance, the sentences "Amyloid beta levels are associated with Alzheimer's Disease stage." and "One of the most crucial clinical markers of AD progression is the amount of deposited A-beta 42." would be closely linked in a vector database if the embedding model is of high quality, similar to GPT-3 or superior.
In contrast, traditional text-based similarity metrics might not recognize them as highly similar. 

$$
\text{Symbols:} \\
\text{LLM} - \text{Large Language Model} \\
\text{AD} - \text{Alzheimer's Disease}
$$

By comparing the user’s question to prior knowledge in the vector database, we can extract the relevant pieces of information from the entire background.
Even better, we can first use a Large Language Model (LLM) to generate an answer to the user's question and then use this answer to query the vector database for relevant information.
Regardless of whether the initial answer is correct, it is likely that the "fake answer" is more semantically similar to the relevant pieces of information than the user's question [@doi:10.48550/arXiv.2308.07107].
Semantic search results (for instance, single sentences directly related to the topic of the question) are then sufficiently small to be added to the prompt.
In this way, the model can learn from additional context without the need for retraining or fine-tuning.
This method is sometimes described as in-context learning [@doi:10.48550/arxiv.2303.17580] or retrieval-augmented generation [@rag].

$$
\text{LLM: Large Language Model}
$$

To provide access to this functionality in BioChatter, we implement classes for the connection to, and management of, vector database systems (in the `vectorstore.py` module), and for performing semantic search on the vector database and injecting the results into the prompt (in the `vectorstore_agent.py` module).
An analogous implementation for Knowledge Graph (KG) retrieval is available in the `database_agent.py` module.

Both retrieval mechanisms are integrated and provided to the BioChatter API via the `rag_agent.py` module.
To demonstrate the use of the API, we add a “Retrieval-Augmented Generation” tab to the preview apps that allows the upload of text documents to be added to a vector database, which then can be queried to add contextual information to the prompt sent to the primary model.
This contextual information is transparently displayed.

Since this functionality requires a connection to a vector database system, we provide connectivity to a Milvus service, including a way to start the service in conjunction with a BioCypher knowledge graph and the BioChatter Light app in one Docker Compose workflow.

An example use case of this functionality is available in [Supplementary Note 2: Retrieval-Augmented Generation] and on our website ([https://biochatter.org/vignette-rag/](https://biochatter.org/vignette-rag/)).

### Deployment of Open-Source Models

To facilitate access to open-source models, we adopted a flexible deployment framework based on the Xorbits Inference API [@{https://github.com/xorbitsai/inference}].
Xorbits Inference includes a large number of open-source models out of the box, and new models from Hugging Face Hub [@{https://huggingface.co/}] can be added using the intuitive graphical user interface.
We utilized Xorbits Inference version 0.8.4 to deploy the benchmarked models, and we provide a Docker Compose repository to deploy the app on a Linux server with Nvidia GPUs ([https://github.com/biocypher/xinference-docker-builtin/](https://github.com/biocypher/xinference-docker-builtin/)).
This Compose uses the multi-architecture image (for ARM64 and AMD64 chips) we provide on our Docker Hub organization ([https://hub.docker.com/repository/docker/biocypher/xinference-builtin](https://hub.docker.com/repository/docker/biocypher/xinference-builtin)).

$$
X = Y + Z
$$ {#eq1}

On Mac OS with Apple Silicon chips, Docker does not have access to the GPU driver, and as such, Xinference needs to be deployed natively.

### Model Chaining

The capability of Large Language Models (LLMs) to interact with external software, including other LLMs, presents a multitude of opportunities for orchestrating intricate tasks.
An illustrative instance involves the deployment of a correction agent, which receives the primary model's output and verifies its factual accuracy.
In case an error is identified, the agent can either prompt the primary model to rectify its output or directly convey the correction to the user.
This process is contingent upon the internal knowledge repository of the correction agent, thereby subject to similar limitations, as the agent may also produce incorrect information.
Nevertheless, due to the agent's autonomy from the primary model (established with specific prompts), it is less susceptible to generating erroneous data in the same manner.

$$
\text{Symbols:} \\
LLMs - \text{Large Language Models}
$$

This approach can be extended to a more complex model chain, where the correcting agent, for example, can query a knowledge graph or a vector database to ground its responses in prior knowledge.
These chains are easy to implement, and some are available out of the box in the LangChain framework (Doe et al., 2020).
However, they can behave unpredictably, which increases with the number of links in the chain and, as such, should be tightly controlled.
They also add to the computational burden of the system, which is particularly relevant for deployments on end-user devices.
