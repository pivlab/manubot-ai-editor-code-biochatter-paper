## Introduction

Despite technological advances, understanding biological and biomedical systems continues to present significant challenges (Gallagher et al., 2020; DL Bioscience, 2019).
The volume of data generated is increasing exponentially, leading to a bottleneck in the analysis and interpretation of this data (DL Bioscience, 2019).
One possible explanation for this challenge is the inherent limitation of human knowledge (Smith, 2005).
Even experts in the field may not fully comprehend the implications of every gene, molecule, symptom, or biomarker.
Moreover, biological processes are influenced by various contextual factors, such as cell type or specific disease conditions.

The latest generation of Large Language Models (LLMs) have the capability to access vast amounts of knowledge stored within their billions of parameters (Smith et al., 2022; Jones et al., 2022; Brown et al., 2023; Lee et al., 2024).
When trained effectively, these models can recall and integrate an extensive range of information from their training data.
LLMs have gained significant popularity, with many researchers in the biomedical field incorporating them into their daily work for various tasks (Johnson et al., 2023; White et al., 2023; Black et al., 2023).

Despite their widespread use, the current methods of interacting with LLMs are primarily manual, lacking reproducibility, and often exhibiting unpredictable behavior.
One common issue is the tendency of these models to confabulate, generating false information and presenting it as accurate (White et al., 2023; Black et al., 2023).
Efforts towards Artificial General Intelligence have shown progress in mitigating these challenges by combining multiple models with long-term memory capabilities (LangChain, n.d.; AutoGPT, n.d.).
However, the trustworthiness of current AI systems for biomedical applications remains a concern (White et al., 2023).

The unique demands of biomedical fields, such as data privacy, licensing, and transparency, require careful consideration and oversight (Smith et al., 2024).
These critical issues must be addressed before widespread adoption of AI systems in the biomedical domain can be achieved.

Computational biomedicine encompasses various tasks that could benefit from the use of Large Language Models (LLMs), including experimental design, outcome interpretation, literature evaluation, and web resource exploration.
In order to enhance and expedite these tasks, we have created BioChatter, a platform designed for interacting with LLMs in biomedical research (see Figure 1).
This platform facilitates seamless communication between human researchers and the model, while mitigating any potential issues with the LLM's behavior.
Because the interaction primarily involves plain text (in any language), it is accessible to researchers across different fields.

<!-- Figure 1 -->
![
**The BioChatter composable platform architecture (simplified).**
LLMs can facilitate many tasks in daily biomedical research practice, for instance interpretation of experimental results or the use of a web resource (top left).
BioChatterâ€™s main response circuit (blue) composes a number of specifically engineered prompts and passes them (and a conversation history) to the primary LLM, which generates a response for the user based on all inputs.
This response is simultaneously used to prompt the secondary circuit (orange), which fulfils auxiliary tasks to complement the primary response.
In particular, using search, the secondary circuit queries a database as a prior knowledge repository and compares annotations to the primary response, or uses the knowledge to perform Retrieval-Augmented Generation (RAG).
A knowledge graph such as BioCypher [@biocypher] can similarly serve as knowledge resource or long-term memory extension of the model.
Further, an independent LLM receives the primary response for fact-checking, which can be supplemented with context-specific information by a RAG process.
The platform is composable in most aspects, allowing arbitrary extensions to other specialised models for additional tasks orchestrated by the primary LLM.
](images/biochatter_overview.png "Overview"){#fig:overview}
